{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxehc7pmA1gDRi0R0JaEF1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreekarvamsi/Model_Finetuning_n_Quantization/blob/main/Quantized_BERT_for_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project: Quantized BERT for Text Classification"
      ],
      "metadata": {
        "id": "3hyXK10yd4Io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to demonstrate the benefits of post-training quantization on a fine-tuned BERT model. We'll fine-tune bert-base-uncased for sentiment analysis, quantize it to INT8, and then compare the model size, inference speed, and accuracy before and after quantization.\n"
      ],
      "metadata": {
        "id": "vnTZTM7MdyMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setup and Installation\n",
        "First, let's install the necessary libraries. We'll use Hugging Face's transformers for the model, datasets for the data, evaluate for metrics, and optimum for the quantization process with ONNX Runtime."
      ],
      "metadata": {
        "id": "tJ4uSAcId89k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tsOCZykodn1J"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch] datasets evaluate optimum[onnxruntime]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load and Prepare the Dataset\n",
        "We'll use the IMDB dataset, a classic benchmark for binary text classification (positive/negative movie reviews).\n",
        "\n",
        "Load the Dataset: We'll load the dataset and create a smaller subset for faster fine-tuning, which is ideal for a one-day project.\n",
        "\n",
        "Load Tokenizer: Load the bert-base-uncased tokenizer to preprocess our text.\n",
        "\n",
        "Tokenize Data: Apply the tokenizer to the dataset."
      ],
      "metadata": {
        "id": "HkM7SVaPeQA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load dataset and create a smaller sample for quick training\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "small_train_dataset = imdb_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_test_dataset = imdb_dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "# Apply tokenizer to the datasets\n",
        "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eW2G6aIEeLSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Fine-Tune the BERT Model\n",
        "Now, we'll fine-tune the standard bert-base-uncased model on our prepared dataset."
      ],
      "metadata": {
        "id": "NK51n1e3eZiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=2\n",
        ")\n",
        "\n",
        "# Define directory to save the model\n",
        "FP32_MODEL_DIR = \"./models/bert-fp32\"\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=FP32_MODEL_DIR,\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Define metrics\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Create Trainer and start fine-tuning\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the final fine-tuned model\n",
        "trainer.save_model(FP32_MODEL_DIR)"
      ],
      "metadata": {
        "id": "2-ykeRTNedrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lR ./models/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dJtbC5YViH4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Baseline Evaluation (FP32 Model)\n",
        "Before quantizing, we need to measure the performance of our original, full-precision (FP32) model.\n",
        "\n",
        "Model Size: Check the size of the saved pytorch_model.bin file.\n",
        "\n",
        "Inference Latency: Time how long it takes to run predictions on the test set.\n",
        "\n",
        "Accuracy: Evaluate the model's accuracy on the test set."
      ],
      "metadata": {
        "id": "VpYQFgNF1bSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Measure Model Size\n",
        "fp32_model_size = os.path.getsize(os.path.join(FP32_MODEL_DIR, \"model.safetensors\")) / (1024 * 1024)\n",
        "print(f\"FP32 Model Size: {fp32_model_size:.2f} MB\")\n",
        "\n",
        "# 2. Measure Inference Latency and Accuracy\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "total_time = 0\n",
        "correct_predictions = 0\n",
        "num_samples = len(tokenized_test)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(num_samples):\n",
        "        # inputs = {k: v.to(device).unsqueeze(0) for k, v in tokenized_test[i].items() if k in tokenizer.model_input_names}\n",
        "        # ✅ This is the corrected code\n",
        "        inputs = {k: torch.tensor(v).to(device).unsqueeze(0) for k, v in tokenized_test[i].items() if k in tokenizer.model_input_names}\n",
        "        start_time = time.time()\n",
        "        outputs = model(**inputs)\n",
        "        total_time += time.time() - start_time\n",
        "\n",
        "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "        if prediction == tokenized_test[i][\"label\"]:\n",
        "            correct_predictions += 1\n",
        "\n",
        "fp32_latency = (total_time / num_samples) * 1000 # Average latency in ms\n",
        "fp32_accuracy = correct_predictions / num_samples\n",
        "print(f\"FP32 Average Latency: {fp32_latency:.2f} ms\")\n",
        "print(f\"FP32 Accuracy: {fp32_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "VxN32BMW1bSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the human-readable labels in the correct order (0: NEGATIVE, 1: POSITIVE)\n",
        "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
        "\n",
        "def predict(text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Takes a text sentence and a model, and returns the predicted sentiment.\n",
        "    \"\"\"\n",
        "    # 1. Tokenize the input text and convert to tensors\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # 2. Run inference\n",
        "    # The .no_grad() is important as we are not training\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # 3. Get the prediction\n",
        "    # The raw output is logits; torch.argmax finds the index of the highest score\n",
        "    prediction_index = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "    # 4. Decode the prediction\n",
        "    return labels[prediction_index]"
      ],
      "metadata": {
        "id": "FJP-0nNC5LBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define some test sentences ---\n",
        "positive_sentence = \"I absolutely loved this movie, the acting was brilliant and the story was gripping!\"\n",
        "negative_sentence = \"This was a complete waste of time. The plot was predictable and the characters were boring.\"\n",
        "\n",
        "# --- Test the Original FP32 Model ---\n",
        "# Ensure your FP32 'model' is loaded and on the CPU for a fair comparison if needed\n",
        "# model.to(\"cpu\")\n",
        "fp32_prediction_pos = predict(positive_sentence, model, tokenizer)\n",
        "fp32_prediction_neg = predict(negative_sentence, model, tokenizer)\n",
        "\n",
        "print(\"--- Testing FP32 PyTorch Model ---\")\n",
        "print(f\"Sentence: '{positive_sentence}'\")\n",
        "print(f\"Prediction: {fp32_prediction_pos}\") # Expected: POSITIVE\n",
        "print(\"-\" * 20)\n",
        "print(f\"Sentence: '{negative_sentence}'\")\n",
        "print(f\"Prediction: {fp32_prediction_neg}\") # Expected: NEGATIVE\n",
        "print(\"\\n\" + \"=\"*40 + \"\\n\")"
      ],
      "metadata": {
        "id": "06FZYRSc4zXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Apply Post-Training Quantization (INT8)\n",
        "Now for the core step. We'll use the optimum library to easily convert our PyTorch model to a quantized ONNX model. We will use Post-Training Dynamic Quantization, where the model weights are converted to INT8.\n",
        "\n",
        "We added a new Export step that uses ORTModelForSequenceClassification to convert your PyTorch model from FP32_MODEL_DIR.\n",
        "\n",
        "We save this new ONNX version to a different directory, ONNX_FP32_MODEL_DIR.\n",
        "\n",
        "Crucially, the ORTQuantizer is now loaded from this new ONNX directory, which contains the .onnx file it needs.\n",
        "\n"
      ],
      "metadata": {
        "id": "o7hOsUJc1i9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.exporters.onnx import main_export\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. EXPORT the fine-tuned PyTorch model to ONNX format ---\n",
        "\n",
        "# Define where to save the ONNX model\n",
        "ONNX_FP32_MODEL_DIR = Path(\"./models/bert-onnx-fp32\")\n",
        "\n",
        "# Use the main_export function for more control\n",
        "main_export(\n",
        "    model_name_or_path=FP32_MODEL_DIR,\n",
        "    output=ONNX_FP32_MODEL_DIR,\n",
        "    task=\"text-classification\",  # We explicitly define the task here\n",
        "    opset=14,                    # And we explicitly set the opset version\n",
        ")\n",
        "\n",
        "# Note: The tokenizer is not automatically saved with main_export, so we save it manually.\n",
        "tokenizer.save_pretrained(ONNX_FP32_MODEL_DIR)\n",
        "\n",
        "print(f\"PyTorch model exported to ONNX format at: {ONNX_FP32_MODEL_DIR}\")\n",
        "\n",
        "\n",
        "# --- 2. QUANTIZE the exported ONNX model ---\n",
        "# (The quantization code from before remains the same)\n",
        "\n",
        "from optimum.onnxruntime import ORTQuantizer, AutoQuantizationConfig\n",
        "\n",
        "INT8_MODEL_DIR = \"./models/bert-int8\"\n",
        "\n",
        "# Create a quantizer FROM THE NEW ONNX MODEL DIRECTORY\n",
        "quantizer = ORTQuantizer.from_pretrained(ONNX_FP32_MODEL_DIR)\n",
        "\n",
        "# Define the quantization configuration\n",
        "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
        "\n",
        "# Apply quantization\n",
        "quantizer.quantize(\n",
        "    save_dir=INT8_MODEL_DIR,\n",
        "    quantization_config=dqconfig,\n",
        ")\n",
        "\n",
        "print(f\"Quantized INT8 model saved to: {INT8_MODEL_DIR}\")"
      ],
      "metadata": {
        "id": "1bTBSb2v1i9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: Evaluate the Quantized Model (INT8)\n",
        "Finally, we evaluate the quantized INT8 model and compare its performance to the FP32 baseline."
      ],
      "metadata": {
        "id": "du-pgN3o1i9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import time\n",
        "# from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "\n",
        "# # 1. Measure Model Size\n",
        "# int8_model_size = os.path.getsize(os.path.join(INT8_MODEL_DIR, \"model_quantized.onnx\")) / (1024 * 1024)\n",
        "# print(f\"INT8 Quantized Model Size: {int8_model_size:.2f} MB\")\n",
        "\n",
        "# # 2. Measure Inference Latency and Accuracy\n",
        "# quantized_model = ORTModelForSequenceClassification.from_pretrained(INT8_MODEL_DIR)\n",
        "\n",
        "# total_time_quantized = 0\n",
        "# correct_predictions_quantized = 0\n",
        "# num_samples = len(tokenized_test)\n",
        "\n",
        "# for i in range(num_samples):\n",
        "#     # The fix is applied on the next line\n",
        "#     inputs = {k: torch.tensor(v).unsqueeze(0) for k, v in tokenized_test[i].items() if k in tokenizer.model_input_names}\n",
        "#     start_time = time.time()\n",
        "#     outputs = quantized_model(**inputs)\n",
        "#     total_time_quantized += time.time() - start_time\n",
        "\n",
        "#     prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "#     if prediction == tokenized_test[i][\"label\"]:\n",
        "#         correct_predictions_quantized += 1\n",
        "\n",
        "# int8_latency = (total_time_quantized / num_samples) * 1000\n",
        "# int8_accuracy = correct_predictions_quantized / num_samples\n",
        "\n",
        "# print(f\"INT8 Average Latency: {int8_latency:.2f} ms\")\n",
        "# print(f\"INT8 Accuracy: {int8_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "_JX2N0wg1i9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# 1. Measure Model Size\n",
        "int8_model_size = os.path.getsize(os.path.join(INT8_MODEL_DIR, \"model_quantized.onnx\")) / (1024 * 1024)\n",
        "print(f\"INT8 Quantized Model Size: {int8_model_size:.2f} MB\")\n",
        "\n",
        "# 2. Measure Inference Latency and Accuracy using BATCHES\n",
        "quantized_model = ORTModelForSequenceClassification.from_pretrained(INT8_MODEL_DIR)\n",
        "\n",
        "# --- FIX 1: Add token_type_ids to the DataLoader ---\n",
        "batch_size = 32\n",
        "input_ids = torch.tensor([item['input_ids'] for item in tokenized_test])\n",
        "attention_mask = torch.tensor([item['attention_mask'] for item in tokenized_test])\n",
        "token_type_ids = torch.tensor([item['token_type_ids'] for item in tokenized_test]) # Added this line\n",
        "labels = torch.tensor([item['label'] for item in tokenized_test])\n",
        "# Add token_type_ids to the dataset\n",
        "eval_dataset = TensorDataset(input_ids, attention_mask, token_type_ids, labels)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n",
        "# ---\n",
        "\n",
        "total_time_quantized = 0\n",
        "correct_predictions_quantized = 0\n",
        "\n",
        "for batch in eval_dataloader:\n",
        "    # --- FIX 2: Unpack the new token_type_ids and add to the inputs dict ---\n",
        "    batch_input_ids, batch_attention_mask, batch_token_type_ids, batch_labels = batch\n",
        "\n",
        "    inputs = {\n",
        "        \"input_ids\": batch_input_ids,\n",
        "        \"attention_mask\": batch_attention_mask,\n",
        "        \"token_type_ids\": batch_token_type_ids, # Added this line\n",
        "    }\n",
        "    # ---\n",
        "\n",
        "    start_time = time.time()\n",
        "    outputs = quantized_model(**inputs)\n",
        "    total_time_quantized += time.time() - start_time\n",
        "\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    correct_predictions_quantized += torch.sum(predictions == batch_labels).item()\n",
        "\n",
        "# --- Calculate final metrics ---\n",
        "num_samples = len(tokenized_test)\n",
        "int8_latency = (total_time_quantized / num_samples) * 1000\n",
        "int8_accuracy = correct_predictions_quantized / num_samples\n",
        "\n",
        "print(f\"INT8 Average Latency: {int8_latency:.2f} ms\")\n",
        "print(f\"INT8 Accuracy: {int8_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "UCAXioPO1yHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define some test sentences ---\n",
        "positive_sentence = \"I absolutely loved this movie, the acting was brilliant and the story was gripping!\"\n",
        "negative_sentence = \"This was a complete waste of time. The plot was predictable and the characters were boring.\"\n",
        "\n",
        "\n",
        "# --- Test the Quantized INT8 Model ---\n",
        "# Ensure your 'quantized_model' is loaded\n",
        "int8_prediction_pos = predict(positive_sentence, quantized_model, tokenizer)\n",
        "int8_prediction_neg = predict(negative_sentence, quantized_model, tokenizer)\n",
        "\n",
        "print(\"--- Testing INT8 ONNX Model ---\")\n",
        "print(f\"Sentence: '{positive_sentence}'\")\n",
        "print(f\"Prediction: {int8_prediction_pos}\") # Expected: POSITIVE\n",
        "print(\"-\" * 20)\n",
        "print(f\"Sentence: '{negative_sentence}'\")\n",
        "print(f\"Prediction: {int8_prediction_neg}\") # Expected: NEGATIVE"
      ],
      "metadata": {
        "id": "NR8IRPLD4d8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 7: Final Results\n",
        "Now, compile and present the final comparison as requested."
      ],
      "metadata": {
        "id": "_T1gOIBGfV8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the performance changes\n",
        "size_reduction = fp32_model_size / int8_model_size\n",
        "latency_reduction = fp32_latency / int8_latency\n",
        "accuracy_drop = (fp32_accuracy - int8_accuracy) * 100\n",
        "\n",
        "print(\"---\" * 10)\n",
        "print(\"✅ Project Results Summary ✅\")\n",
        "print(\"---\" * 10)\n",
        "print(f\"Reduced model size by {size_reduction:.2f}x (from {fp32_model_size:.2f} MB to {int8_model_size:.2f} MB).\")\n",
        "print(f\"Reduced inference latency by {latency_reduction:.2f}x (from {fp32_latency:.2f} ms to {int8_latency:.2f} ms).\")\n",
        "print(f\"Accuracy drop: {accuracy_drop:.2f}%.\")\n",
        "print(\"---\" * 10)"
      ],
      "metadata": {
        "id": "QaHhVsnrfRtJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}